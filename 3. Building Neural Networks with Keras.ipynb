{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building Neural Networks with Keras\n",
    "![LISA logo](https://raw.githubusercontent.com/wshand/Python-Data-Science-Workshop/master/assets/LISA_logo_medium.jpg)\n",
    "\n",
    "This notebook helps introduce some of the most basic tools that are commonly used for doing data science and statistics in Python.\n",
    "\n",
    "# Table of contents\n",
    "* [Introduction](#introduction)\n",
    "* [Additional resources](#additional-resources)\n",
    "\n",
    "# Introduction <a id=\"introduction\"></a>\n",
    "One of the biggest developments in machine learning in recent years is the usage of [_**artificial neural networks**_](https://en.wikipedia.org/wiki/Artificial_neural_network) (ANNs) as a powerful tool in the data scientist's toolbox. Although neural nets have been around for over 70 years, they've only become popular in the past decade thanks to advancements in algorithms and hardware.\n",
    "\n",
    "On the algorithms side, [backpropagation](https://en.wikipedia.org/wiki/Backpropagation) (aka backwards-mode automatic differentiation), introduced in the 1980s, made it much easier to train neural networks. Later, researchers introduced neural network architectures such as [convolutional networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNNs) and [LSTMs](https://en.wikipedia.org/wiki/Long_short-term_memory) that saw success in tasks such as handwriting recognition.\n",
    "\n",
    "Meanwhile, hardware limitations originally made it difficult to train neural networks, which require more data and computing power than most machine learning methods. However, improvements in processor power and the advent of specialized hardware such as graphics processing units have made training neural nets much faster. Massively parallel computing has also enabled boosts in training time: for instance, Google's [AlphaZero](https://en.wikipedia.org/wiki/AlphaZero) was trained on 5,000 tensor processing units in parallel in just a few hours.\n",
    "\n",
    "*Talk about recent achievements of NNs*\n",
    "\n",
    "In this workshop, we will be using [Keras](https://keras.io) to build our own neural networks. Keras is a high-level interface for building ANNs, with libraries like [TensorFlow](https://www.tensorflow.org) running on the backend.\n",
    "\n",
    "### Note: you will need to run this code cell every time you restart this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy             as np\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras          import backend\n",
    "from keras.utils    import to_categorical\n",
    "from keras.layers   import Dense, Dropout\n",
    "from keras.layers   import Conv2D, MaxPooling2D, Flatten\n",
    "from keras.models   import Sequential\n",
    "\n",
    "########## Helper functions\n",
    "def square_axes(ax, data, expansion_factor=1.05):\n",
    "    # Change limits of plot axes to center on the input dataset, and to put the\n",
    "    # x-axis and y-axis on the same scale\n",
    "    m        = np.mean(data)\n",
    "    max_dist = max([np.linalg.norm(u-m) for u in data]) * expansion_factor\n",
    "    lims     = [m-max_dist, m+max_dist]\n",
    "    try:    ax.set_xlim(lims); ax.set_ylim(lims)\n",
    "    except: ax.xlim(lims); ax.ylim(lims)\n",
    "\n",
    "def plot_decision_boundary(X, clf, ax, incr=1, h=.02):\n",
    "    # Plot the support vector machine decision boundary for 2D data\n",
    "    xmin, xmax = X[:,0].min()-incr, X[:,0].max()+incr\n",
    "    ymin, ymax = X[:,1].min()-incr, X[:,1].max()+incr\n",
    "    xx, yy     = np.meshgrid(np.arange(xmin,xmax,h),np.arange(ymin,ymax,h))\n",
    "    Z          = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=.2)\n",
    "\n",
    "def load_mnist_wrapper(n_classes=10):\n",
    "    # Loads the MNIST dataset, and does some preprocessing\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    # Only keep the first n_classes digits\n",
    "    X_train = X_train[y_train < n_classes]\n",
    "    y_train = y_train[y_train < n_classes]\n",
    "    X_test  = X_test[y_test < n_classes]\n",
    "    y_test  = y_test[y_test < n_classes]\n",
    "    \n",
    "    # Pre-process the data for the backend we're using\n",
    "    if backend.image_data_format() == 'channels_first':\n",
    "        X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "        X_test  = X_test.reshape(X_test.shape[0],   1, 28, 28)\n",
    "    else:\n",
    "        X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "        X_test  = X_test.reshape(X_test.shape[0],   28, 28, 1)\n",
    "\n",
    "    # Change pixel intensities from the range 0 - 255 to the range 0 - 1\n",
    "    X_train = X_train / 255\n",
    "    X_test  = X_test  / 255\n",
    "\n",
    "    # Convert the class labels into a format that Keras will be able to parse\n",
    "    y_train = to_categorical(y_train, n_mnist_classes)\n",
    "    y_test  = to_categorical(y_test, n_mnist_classes)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "##########################\n",
    "\n",
    "# Constants\n",
    "n_mnist_classes = 3\n",
    "\n",
    "# Pre-load data\n",
    "(X_train, y_train), (X_test, y_test) = load_mnist_wrapper(n_classes=n_mnist_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are neural networks?\n",
    "*Intro to neural nets*\n",
    "\n",
    "Let's try building our first neural net. We're going to have a dataset that consists of two circles, one inscribed within the other. The points on the interior circle will be in the first class, and the points on the exterior circle will be in the second class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, classes = make_circles(n_samples=200, factor=0.3, noise=0.1)\n",
    "plt.scatter(X[classes == 0,0], X[classes == 0,1], label=\"Class 0\")\n",
    "plt.scatter(X[classes == 1,0], X[classes == 1,1], label=\"Class 1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network will consist of an input layer, one hidden layer, and an output layer. Since our data are two-dimensional, we will have two neurons in the input layer (one for $x$ coordinates, another for $y$ coordinates). We'll put $8$ neurons in the hidden layer, and one neuron in the output layer. The output of the network will be an estimated probability that the data point is in the class on the interior circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't want to have to repeatedly call the `add` function, you can instead tell Keras what layers you want at the same time as you define your neural net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the network! Our first step is to call `model.compile`. The `compile` function allows us to make some final configurations before we start training. We can do things like\n",
    "\n",
    "* Set which [optimizer](https://keras.io/optimizers/) we'll use\n",
    "* Choose a [loss function](https://keras.io/losses/), which helps us score how good our neural net is\n",
    "* Set options that will be used on the backend (in this case, by TensorFlow) beneath the Keras API.\n",
    "\n",
    "For more information on compilation options, see the [Keras](https://keras.io/getting-started/sequential-model-guide/#compilation) [documentation](https://keras.io/models/model/#compile).\n",
    "\n",
    "Now that the model has been compiled, you can run `model.fit` to actually train the neural network. When you call `model.fit`, you provide a few parameters (such as `epochs` and `steps_per_epoch`) that tell Keras how much training you want to do. In the code below, we do $5$ training epochs each consisting of $512$ training steps, totalling $5\\times 512 = 2560$ steps of neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Options:\n",
    "#    loss='binary_crossentropy'\n",
    "#      - Loss function for classification problems with two classes.\n",
    "#      - If we had more than one class, we'd use 'categorical_crossentropy' instead\n",
    "#\n",
    "#    optimizer='rmsprop'\n",
    "#      - Tells Keras to use the RMSProp optimization routine\n",
    "#      - See https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\n",
    "#\n",
    "#    metrics=['accuracy']\n",
    "#      - When we're training our neural net, Keras will tell us how accurate the net\n",
    "#        currently is.\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "# model.fit takes two numpy arrays. The first is the array X, which contains the actual\n",
    "# data. The second is an array y that contains the predictions we are trying to make. Since\n",
    "# this is a classification problem with two classes (\"blue\" and \"orange\"), y should contain\n",
    "# 0's and 1's (where 0 = blue, 1 = orange).\n",
    "#\n",
    "# Options:\n",
    "#     epochs=5\n",
    "#       - Each 'epoch' can be thought of as a unit of training, consisting of\n",
    "#         many small training steps\n",
    "#       - The number of training steps in each epoch is controlled by \n",
    "#         steps_per_epoch\n",
    "#\n",
    "#     steps_per_epoch=512\n",
    "#       - Number of training steps to do in each epoch\n",
    "#       - The total number of training steps done when model.fit is called is \n",
    "#         epochs * steps_per_epoch\n",
    "#     \n",
    "#     verbose=1\n",
    "#       - Tells Keras to show a progress bar for each training epoch that keeps us\n",
    "#         updated on how much the network has been trained.\n",
    "#\n",
    "model.fit(X, classes, epochs=5, steps_per_epoch=512, verbose=1)\n",
    "\n",
    "# Plot data, and the decision boundary found by the network\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,6))\n",
    "for ax in axes:\n",
    "    ax.scatter(X[classes == 0,0], X[classes == 0,1])\n",
    "    ax.scatter(X[classes == 1,0], X[classes == 1,1])\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "plot_decision_boundary(X, model, axes[1], incr=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your network has been trained, you can make predictions using `model.predict`, e.g.\n",
    "\n",
    "```python\n",
    "predictions = model.predict(Z)  # Z is some numpy array with data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building convolutional networks for the MNIST dataset\n",
    "In this part of the workshop, we're going to start looking at how neural networks are used in computer vision. We're going to use the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database), a classic computer vision dataset containing images of tens of thousands of handwritten digits. The MNIST dataset can actually be loaded into Python using Keras:\n",
    "\n",
    "```python\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "```\n",
    "\n",
    "We will build a neural network that will be able to look at an MNIST digit tell us what number it sees. To help shorten training times we're only going to look at the digits $0$, $1$, and $2$; you can build a network that classifies more digits by increasing `n_mnist_classes` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "### For purpose of demonstration we're not going to use all of  ########\n",
    "### the digits. Increase n_mnist classes if you want to try     ########\n",
    "### training your neural net on more types of digits.           ########\n",
    "########################################################################\n",
    "\n",
    "n_mnist_classes = 3\n",
    "\n",
    "########################################################################\n",
    "########################################################################\n",
    "\n",
    "# load_mnist_wrapper brings in the MNIST data with keras.datasets.mnist.load_data()\n",
    "# and does some useful preprocessing. Check out the first code cell of this notebook\n",
    "# to see how this works.\n",
    "(X_train, y_train), (X_test, y_test) = load_mnist_wrapper(n_classes=n_mnist_classes)\n",
    "    \n",
    "# Show a few randomly selected images\n",
    "fig, axes = plt.subplots(3, 5, figsize=(8,8))\n",
    "\n",
    "c = np.random.choice(X_train.shape[0], axes.size)\n",
    "digits, classes = X_train[c], y_train[c]\n",
    "\n",
    "for (ax,img,num) in zip(axes.flatten(), digits, classes):\n",
    "    ax.imshow(img.reshape((28,28)), cmap='gray')\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.set_xlabel(\"Digit: \" + str(num.argmax()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could apply a plain feedforward neural network to this dataset, and you'd get decent results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from keras.utils  import to_categorical\n",
    "from keras        import backend\n",
    "\n",
    "# Slight modification of architecture from the following example in the\n",
    "# Keras repository:\n",
    "# https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
    "mnist_model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3,3), activation='relu'),\n",
    "    Conv2D(32, (3,3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Dropout(.25),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(.5),\n",
    "    Dense(n_mnist_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "mnist_model.compile(loss='categorical_crossentropy', optimizer='adadelta',\n",
    "                    metrics=['accuracy'])\n",
    "mnist_model.fit(X_train, y_train, verbose=1, batch_size=256,\n",
    "                epochs=1, validation_data=(X_test, y_test))\n",
    "\n",
    "score = mnist_model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss: %.4f'     % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])\n",
    "\n",
    "# If we didn't get 100% classification accuracy, show some images that we\n",
    "# misclassified\n",
    "predictions = mnist_model.predict(X_test).argmax(axis=1)\n",
    "mclf_idx    = (predictions != y_test.argmax(axis=1)).flatten()\n",
    "X_mclf      = X_test[mclf_idx]\n",
    "y_mclf      = y_test[mclf_idx]\n",
    "pred_mclf   = predictions[mclf_idx]\n",
    "\n",
    "print(\"On test set, misclassified %d out of %d\" % (X_mclf.shape[0], predictions.size))\n",
    "\n",
    "if X_mclf.shape[0] >= 3:\n",
    "    fig, axes   = plt.subplots(1,3,figsize=(8,8))\n",
    "    c           = np.random.choice(X_mclf.shape[0], 3, replace=False)\n",
    "    for (ii,ax) in enumerate(axes):\n",
    "        ax.imshow(X_mclf[c[ii]].reshape((28,28)), cmap='gray')\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        pred_class, true_class = predictions[c[ii]], y_mclf[c[ii]].argmax()\n",
    "        ax.set_xlabel(\"Predicted: \" + str(pred_class) + \n",
    "                      \"\\nTrue class: \" + str(true_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional resources <a id=\"additional-resources\"></a>\n",
    "* [playground.tensorflow.org](https://playground.tensorflow.org/) allows you to experiment with some simple neural nets.\n",
    "* [Hacker's guide to Neural Networks](https://karpathy.github.io/neuralnets/)\n",
    "* References for specific ANN architectures:\n",
    "  * Convolutional networks\n",
    "  * LSTMs\n",
    "    * [Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Workshop-Kernel",
   "language": "python",
   "name": "workshop-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
