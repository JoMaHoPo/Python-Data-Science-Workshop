{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building Neural Networks with Keras\n",
    "![LISA logo](https://raw.githubusercontent.com/wshand/Python-Data-Science-Workshop/master/assets/LISA_logo_medium.jpg)\n",
    "\n",
    "This notebook helps introduce some of the most basic tools that are commonly used for doing data science and statistics in Python.\n",
    "\n",
    "# Table of contents\n",
    "* [Introduction](#introduction)\n",
    "* [What are neural networks?](#intro-neural-nets)\n",
    "* [Adding regularization](#regularization)\n",
    "* [Saving and loading models](#saving)\n",
    "* [Additional resources](#additional-resources)\n",
    "\n",
    "# Introduction <a id=\"introduction\"></a>\n",
    "One of the biggest developments in machine learning in recent years is the usage of [_**artificial neural networks**_](https://en.wikipedia.org/wiki/Artificial_neural_network) (ANNs) as a powerful tool in the data scientist's toolbox. Although neural nets have been around for over 70 years, they've only become popular in the past decade thanks to advancements in algorithms and hardware.\n",
    "\n",
    "On the algorithms side, [backpropagation](https://en.wikipedia.org/wiki/Backpropagation) (aka backwards-mode automatic differentiation), introduced in the 1980s, made it much easier to train neural networks. Later, researchers introduced neural network architectures such as [convolutional networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNNs) and [LSTMs](https://en.wikipedia.org/wiki/Long_short-term_memory) that saw success in tasks such as handwriting recognition.\n",
    "\n",
    "Meanwhile, hardware limitations originally made it difficult to train neural networks, which require more data and computing power than most machine learning methods. However, improvements in processor power and the advent of specialized hardware such as graphics processing units have made training neural nets much faster. Massively parallel computing has also enabled boosts in training time: for instance, Google's [AlphaZero](https://en.wikipedia.org/wiki/AlphaZero) was trained on 5,000 tensor processing units in parallel in just a few hours.\n",
    "\n",
    "*Talk about recent achievements of NNs*\n",
    "\n",
    "In this workshop, we will be using [Keras](https://keras.io) to build our own neural networks. Keras is a high-level interface for building ANNs, with libraries like [TensorFlow](https://www.tensorflow.org) running on the backend.\n",
    "\n",
    "### Note: you will need to run this code cell every time you restart this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### RUN THIS CELL BEFORE USING THE REST OF THE NOTEBOOK\n",
    "###\n",
    "# Set the Keras backend to be TensorFlow\n",
    "import os, shutil\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy             as np\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras          import backend\n",
    "from keras.utils    import to_categorical\n",
    "from keras.layers   import Dense, Dropout\n",
    "from keras.layers   import Conv2D, MaxPooling2D, Flatten\n",
    "from keras.models   import Sequential, load_model\n",
    "\n",
    "########## Helper functions\n",
    "def square_axes(ax, data, expansion_factor=1.05):\n",
    "    # Change limits of plot axes to center on the input dataset, and to put the\n",
    "    # x-axis and y-axis on the same scale\n",
    "    m        = np.mean(data)\n",
    "    max_dist = max([np.linalg.norm(u-m) for u in data]) * expansion_factor\n",
    "    lims     = [m-max_dist, m+max_dist]\n",
    "    try:    ax.set_xlim(lims); ax.set_ylim(lims)\n",
    "    except: ax.xlim(lims); ax.ylim(lims)\n",
    "\n",
    "def plot_decision_boundary(X, clf, ax, incr=1, h=.02):\n",
    "    # Plot the support vector machine decision boundary for 2D data\n",
    "    xmin, xmax = X[:,0].min()-incr, X[:,0].max()+incr\n",
    "    ymin, ymax = X[:,1].min()-incr, X[:,1].max()+incr\n",
    "    xx, yy     = np.meshgrid(np.arange(xmin,xmax,h),np.arange(ymin,ymax,h))\n",
    "    Z          = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=.2)\n",
    "\n",
    "def load_mnist_wrapper(n_classes=10):\n",
    "    # Loads the MNIST dataset, and does some preprocessing\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    # Only keep the first n_classes digits\n",
    "    X_train = X_train[y_train < n_classes]; y_train = y_train[y_train < n_classes]\n",
    "    X_test  = X_test[y_test < n_classes];   y_test  = y_test[y_test < n_classes]\n",
    "    \n",
    "    # Pre-process the data for the backend we're using\n",
    "    if backend.image_data_format() == 'channels_first':\n",
    "        X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "        X_test  = X_test.reshape(X_test.shape[0],   1, 28, 28)\n",
    "    else:\n",
    "        X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "        X_test  = X_test.reshape(X_test.shape[0],   28, 28, 1)\n",
    "\n",
    "    # Change pixel intensities from the range 0 - 255 to the range 0 - 1\n",
    "    X_train = X_train / 255\n",
    "    X_test  = X_test  / 255\n",
    "\n",
    "    # Convert the class labels into a format that Keras will be able to parse\n",
    "    y_train = to_categorical(y_train, n_classes)\n",
    "    y_test  = to_categorical(y_test,  n_classes)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "##########################\n",
    "\n",
    "n_mnist_classes = 3\n",
    "\n",
    "# Pre-load data\n",
    "(X_train, y_train), (X_test, y_test) = load_mnist_wrapper(n_classes=n_mnist_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are neural networks? <a id=\"intro-neural-nets\"></a>\n",
    "\n",
    "*Intro to neural nets*\n",
    "\n",
    "Let's try building our first neural net. We're going to have a dataset that consists of two circles, one inscribed within the other. The points on the interior circle will be in the first class, and the points on the exterior circle will be in the second class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, classes = make_circles(n_samples=200, factor=0.3, noise=0.1)\n",
    "plt.scatter(X[classes == 0,0], X[classes == 0,1], label=\"Class 0\")\n",
    "plt.scatter(X[classes == 1,0], X[classes == 1,1], label=\"Class 1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network will consist of an input layer, one hidden layer, and an output layer. Since our data are two-dimensional, we will have two neurons in the input layer (one for $x$ coordinates, another for $y$ coordinates). We'll put $8$ neurons in the hidden layer, and one neuron in the output layer. The output of the network will be an estimated probability that the data point is in the class on the interior circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't want to have to repeatedly call the `add` function, you can instead tell Keras what layers you want at the same time as you define your neural net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the network! Our first step is to call `model.compile`. The `compile` function allows us to make some final configurations before we start training. We can do things like\n",
    "\n",
    "* Set which [optimizer](https://keras.io/optimizers/) we'll use\n",
    "* Choose a [loss function](https://keras.io/losses/), which helps us score how good our neural net is\n",
    "* Set options that will be used on the backend (in this case, by TensorFlow) beneath the Keras API.\n",
    "\n",
    "For more information on compilation options, see the [Keras](https://keras.io/getting-started/sequential-model-guide/#compilation) [documentation](https://keras.io/models/model/#compile).\n",
    "\n",
    "Now that the model has been compiled, you can run `model.fit` to actually train the neural network. When you call `model.fit`, you provide a few parameters (such as `epochs` and `steps_per_epoch`) that tell Keras how much training you want to do. In the code below, we do $5$ training epochs each consisting of $512$ training steps, totalling $5\\times 512 = 2560$ steps of neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Options:\n",
    "#    loss='binary_crossentropy'\n",
    "#      - Loss function for classification problems with two classes.\n",
    "#      - If we had more than one class, we'd use 'categorical_crossentropy' instead\n",
    "#\n",
    "#    optimizer='rmsprop'\n",
    "#      - Tells Keras to use the RMSProp optimization routine\n",
    "#      - See https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\n",
    "#\n",
    "#    metrics=['accuracy']\n",
    "#      - When we're training our neural net, Keras will tell us how accurate the net\n",
    "#        currently is.\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "# model.fit takes two numpy arrays. The first is the array X, which contains the actual\n",
    "# data. The second is an array y that contains the predictions we are trying to make. Since\n",
    "# this is a classification problem with two classes (\"blue\" and \"orange\"), y should contain\n",
    "# 0's and 1's (where 0 = blue, 1 = orange).\n",
    "#\n",
    "# Options:\n",
    "#     epochs=5\n",
    "#       - Each 'epoch' can be thought of as a unit of training, consisting of\n",
    "#         many small training steps\n",
    "#       - The number of training steps in each epoch is controlled by \n",
    "#         steps_per_epoch\n",
    "#\n",
    "#     steps_per_epoch=512\n",
    "#       - Number of training steps to do in each epoch\n",
    "#       - The total number of training steps done when model.fit is called is \n",
    "#         epochs * steps_per_epoch\n",
    "#     \n",
    "#     verbose=1\n",
    "#       - Tells Keras to show a progress bar for each training epoch that keeps us\n",
    "#         updated on how much the network has been trained.\n",
    "#\n",
    "model.fit(X, classes, epochs=5, steps_per_epoch=512, verbose=1)\n",
    "\n",
    "# Plot data, and the decision boundary found by the network\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,6))\n",
    "for ax in axes:\n",
    "    ax.scatter(X[classes == 0,0], X[classes == 0,1])\n",
    "    ax.scatter(X[classes == 1,0], X[classes == 1,1])\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "plot_decision_boundary(X, model, axes[1], incr=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your network has been trained, you can make predictions using `model.predict`, e.g.\n",
    "\n",
    "```python\n",
    "predictions = model.predict(Z)  # Z is some numpy array with data\n",
    "```\n",
    "\n",
    "The previous task is an example of a classification problem: given some data and some labels (e.g. \"inner circle\" and \"outer circle\"), create a neural network that can look at a new point and predict what its label should be. Regression is another common machine learning task. In a regression problem, you must instead predict a *response variable*, which can take on a continuum of values. For instance, all of the following are regression problems:\n",
    "\n",
    "* How well will a student score on a standardized test given their grades in school?\n",
    "* Can we assess how happy a person is (on a scale of 1 - 10) from a sample of their writing?\n",
    "* Given data about air pressure, temperature, and wind speed, can I predict how much rainfall we will receive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(-2, 2, size=(80,1))\n",
    "y = np.sin(np.pi * x) + np.random.normal(scale=.15, size=(x.size,1))\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_dim=1),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(8,  activation='relu'),\n",
    "    Dense(1,  activation='linear')\n",
    "])\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "model.fit(x, y, steps_per_epoch=256, epochs=4)\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "xx  = np.linspace(x.min(), x.max(), num=100)\n",
    "plt.scatter(x,y)\n",
    "plt.plot(xx, model.predict(xx), color='r', label='Neural net predictions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding regularization <a id=\"regularization\"></a>\n",
    "Neural networks are a powerful machine learning method -- occasionally, a little too powerful. In the [second workshop of this series](https://nbviewer.jupyter.org/github/wshand/Python-Data-Science-Workshop/blob/master/2.%20Intro%20to%20Machine%20Learning%20in%20Python%20with%20Scikit-learn.ipynb), we discussed how a machine learning method can get a great score with the data it was trained on and still fail in the real world. Neural networks also do well with training data, but are poor at making predictions for data they've never seen before.\n",
    "\n",
    "As a demonstration, we're going to try another very simple regression problem, like we did in the previous section. However, we're going to use a much more complicated neural net than necessary, consisting of more neurons and layers than we really need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model    import LinearRegression\n",
    "from sklearn.metrics         import mean_squared_error\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "x = np.random.uniform(-1, 1, size=(60,1))\n",
    "y = x + np.random.normal(scale=.15, size=(x.size,1))\n",
    "\n",
    "# Split into training and testing data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
    "\n",
    "# Train a complicated neural net on the data\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_dim=1),\n",
    "    Dense(64,  activation='relu'),\n",
    "    Dense(32,  activation='relu'),\n",
    "    Dense(1,   activation='linear')\n",
    "])\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(x_train, y_train, epochs=3, steps_per_epoch=512, verbose=1)\n",
    "\n",
    "# Perform linear regression on the data\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(x_train, y_train)\n",
    "\n",
    "# Compare ANN and linear model\n",
    "print(\"ANN score: %.4f\"          % mean_squared_error(y_test, model.predict(x_test)))\n",
    "print(\"Linear model score: %.4f\" % mean_squared_error(y_test, linear_model.predict(x_test)))\n",
    "\n",
    "# Plot data and the predicted curve\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "xx  = np.linspace(x.min(), x.max(), num=100).reshape((-1,1))\n",
    "plt.scatter(x, y, color='k', alpha=0.6)\n",
    "plt.plot(xx, linear_model.predict(xx), label=\"Linear model\", color='orange')\n",
    "plt.plot(xx, model.predict(xx), label=\"Neural net predictions\", color='blue')\n",
    "plt.legend()\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the output I got on one run of the code cell above:\n",
    "\n",
    "> ```\n",
    "Epoch 1/3\n",
    "512/512 [==============================] - 7s 15ms/step - loss: 0.0215\n",
    "Epoch 2/3\n",
    "512/512 [==============================] - 2s 3ms/step - loss: 0.0126\n",
    "Epoch 3/3\n",
    "512/512 [==============================] - 2s 3ms/step - loss: 0.0112\n",
    "ANN score: 0.0402\n",
    "Linear model score: 0.0188```\n",
    "\n",
    "In other words, the mean squared error received by the ANN on the data it was trained with ($0.0122$) was almost a quarter of the error it got on the testing data ($0.0402$), which it hadn't seen before. Fitting a line to the same training data with [linear regression](https://en.wikipedia.org/wiki/Linear_regression) got an error on the testing data of $0.0188$, less than half of what the ANN got on that data.\n",
    "\n",
    "The problem of getting a much better score on the training data than the testing data is known as *overfitting*. Neural networks are especially prone to overfitting when they contain too many neurons or layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers       import Dropout\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Same neural net as before, but with some added regularization\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_dim=1, activity_regularizer=l2(0.5)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64,  activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32,  activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1,   activation='linear')\n",
    "])\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(x_train, y_train, epochs=3, steps_per_epoch=512, verbose=1)\n",
    "\n",
    "# Compare ANN and linear model\n",
    "print(\"ANN score: %.4f\"          % mean_squared_error(y_test, model.predict(x_test)))\n",
    "print(\"Linear model score: %.4f\" % mean_squared_error(y_test, linear_model.predict(x_test)))\n",
    "\n",
    "# Plot data and the predicted curve\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "xx  = np.linspace(x.min(), x.max(), num=100).reshape((-1,1))\n",
    "plt.scatter(x, y, color='k', alpha=0.6)\n",
    "plt.plot(xx, linear_model.predict(xx), label=\"Linear model\", color='orange')\n",
    "plt.plot(xx, model.predict(xx), label=\"Neural net predictions\", color='blue')\n",
    "plt.legend()\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With regularization, I got the following scores:\n",
    "    \n",
    "> ```\n",
    "ANN score: 0.0284\n",
    "Linear model score: 0.0188```\n",
    "\n",
    "While my neural net still seems to be overfitting a little bit, it's clearly improved from its first version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building convolutional networks for the MNIST dataset\n",
    "In this part of the workshop, we're going to start looking at how neural networks are used in computer vision. We're going to use the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database), a classic computer vision dataset containing images of tens of thousands of handwritten digits. The MNIST dataset can actually be loaded into Python using Keras:\n",
    "\n",
    "```python\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "```\n",
    "\n",
    "We will build a neural network that will be able to look at an MNIST digit tell us what number it sees. To help shorten training times we're only going to look at the digits $0$, $1$, and $2$; you can build a network that classifies more digits by increasing `n_mnist_classes` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "### For purpose of demonstration we're not going to use all of  ########\n",
    "### the digits. Increase n_mnist classes if you want to try     ########\n",
    "### training your neural net on more types of digits.           ########\n",
    "########################################################################\n",
    "\n",
    "n_mnist_classes = 3\n",
    "\n",
    "########################################################################\n",
    "########################################################################\n",
    "\n",
    "# load_mnist_wrapper brings in the MNIST data with keras.datasets.mnist.load_data()\n",
    "# and does some useful preprocessing. Check out the first code cell of this notebook\n",
    "# to see how this works.\n",
    "(X_train, y_train), (X_test, y_test) = load_mnist_wrapper(n_classes=n_mnist_classes)\n",
    "    \n",
    "# Show a few randomly selected images\n",
    "fig, axes = plt.subplots(3, 5, figsize=(8,8))\n",
    "\n",
    "c = np.random.choice(X_train.shape[0], axes.size)\n",
    "digits, classes = X_train[c], y_train[c]\n",
    "\n",
    "for (ax,img,num) in zip(axes.flatten(), digits, classes):\n",
    "    ax.imshow(img.reshape((28,28)), cmap='gray')\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.set_xlabel(\"Digit: \" + str(num.argmax()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could apply a plain feedforward neural network to this dataset, and you'd get decent results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from keras.utils  import to_categorical\n",
    "from keras        import backend\n",
    "\n",
    "# Slight modification of architecture from the following example in the\n",
    "# Keras repository:\n",
    "# https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
    "mnist_model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3,3), activation='relu'),\n",
    "    Conv2D(32, (3,3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Dropout(.25),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(.5),\n",
    "    Dense(n_mnist_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "mnist_model.compile(loss='categorical_crossentropy', optimizer='adadelta',\n",
    "                    metrics=['accuracy'])\n",
    "mnist_model.fit(X_train, y_train, verbose=1, batch_size=256,\n",
    "                epochs=1, validation_data=(X_test, y_test))\n",
    "\n",
    "score = mnist_model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss: %.4f'     % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])\n",
    "\n",
    "# If we didn't get 100% classification accuracy, show some images that we\n",
    "# misclassified\n",
    "predictions = mnist_model.predict(X_test).argmax(axis=1)\n",
    "mclf_idx    = (predictions != y_test.argmax(axis=1)).flatten()\n",
    "X_mclf      = X_test[mclf_idx]\n",
    "y_mclf      = y_test[mclf_idx]\n",
    "pred_mclf   = predictions[mclf_idx]\n",
    "\n",
    "print(\"On test set, misclassified %d out of %d\" % (X_mclf.shape[0], predictions.size))\n",
    "\n",
    "if X_mclf.shape[0] >= 3:\n",
    "    fig, axes   = plt.subplots(1,3,figsize=(8,8))\n",
    "    c           = np.random.choice(X_mclf.shape[0], 3, replace=False)\n",
    "    for (ii,ax) in enumerate(axes):\n",
    "        ax.imshow(X_mclf[c[ii]].reshape((28,28)), cmap='gray')\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        pred_class, true_class = predictions[c[ii]], y_mclf[c[ii]].argmax()\n",
    "        ax.set_xlabel(\"Predicted: \" + str(pred_class) + \n",
    "                      \"\\nTrue class: \" + str(true_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and loading models <a id=\"saving\"></a>\n",
    "Neural networks are extremely computation- and time-intensive to train - the largest nets have taken days to train on hundreds or thousands of specialized hardware units running in parallel. Keras allows you to save an ANN to a file and then reload it later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper functions ###################################\n",
    "def plot_circle_anns(X, classes, model, loaded_model):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(9,5))\n",
    "    for ax in axes:\n",
    "        ax.scatter(X[classes == 0,0], X[classes == 0,1]); ax.set_xticks([])\n",
    "        ax.scatter(X[classes == 1,0], X[classes == 1,1]); ax.set_yticks([])\n",
    "    plot_decision_boundary(X, model,        axes[0], incr=0.1)\n",
    "    plot_decision_boundary(X, loaded_model, axes[1], incr=0.1)\n",
    "    axes[0].set_title(\"Original model\")\n",
    "    axes[1].set_title(\"Model loaded from file\")\n",
    "    plt.show()\n",
    "#########################################################\n",
    "\n",
    "from keras.models import load_model\n",
    "    \n",
    "X, classes = make_circles(n_samples=200, factor=0.3, noise=0.1)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.fit(X, classes, epochs=5, steps_per_epoch=512)\n",
    "\n",
    "# Save the model as an HDF5 file, which contains\n",
    "# - The architecture of the network\n",
    "# - The ANN weights you got from running model.fit\n",
    "# - The training configuration from model.compile (e.g. loss function,\n",
    "#   optimizer)\n",
    "# - The state of the optimizer, so that you can continue training from where you left off\n",
    "model.save('keras_circles_ann.h5')\n",
    "\n",
    "# Now load model from file\n",
    "loaded_model = load_model('keras_circles_ann.h5')\n",
    "\n",
    "# Show decision boundaries for both ANNs side-by-side, overlaid\n",
    "# on the dataset\n",
    "plot_circle_anns(X, classes, model, loaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything worked correctly, the left and right plots should be identical.\n",
    "\n",
    "One nice thing about `model.save` is that it saves the current training state of the network as well. You can spend some time training a model with `model.fit`, save it, and then come back later and continue training the model. All you have to do is call the `fit` function again, e.g. as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do five more epochs of training with the model that was loaded\n",
    "# from the save file\n",
    "loaded_model.fit(X, classes, epochs=5, steps_per_epoch=512)\n",
    "\n",
    "plot_circle_anns(X, classes, model, loaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's increasingly possible to download a neural net someone else has trained off the web, load it, and then start using it yourself. For instance, I trained a neural net for the entire MNIST dataset (all 10 digits) using [some example code](https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py) and added it to the GitHub repository for this workshop. The code cell below downloads this model from the repository (if it isn't already downloaded) and scores it on the MNIST test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "### This code is just to download the model off the ############\n",
    "### GitHub repository if you don't already have it  ############\n",
    "import shutil\n",
    "from urllib.request import urlopen\n",
    "\n",
    "DOWNLOAD_PATH = os.path.join(os.getcwd(), 'mnist_model.h5')\n",
    "REPO_PATH     = os.path.join(os.getcwd(), 'assets', 'models', 'mnist_model.h5')\n",
    "url           = 'https://github.com/wshand/Python-Data-Science-Workshop/blob/'\\\n",
    "                'master/assets/models/mnist_model.h5?raw=true'\n",
    "\n",
    "if not os.path.isfile(REPO_PATH) and not os.path.isfile(DOWNLOAD_PATH):\n",
    "    with urlopen(url) as response, open(DOWNLOAD_PATH, 'wb') as f:\n",
    "        print('Downloading model from', url)\n",
    "        print('Downloading to', DOWNLOAD_PATH)\n",
    "        shutil.copyfileobj(response, f)\n",
    "################################################################\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = load_mnist_wrapper(n_classes=10)\n",
    "if os.path.isfile(REPO_PATH):\n",
    "    model = load_model(REPO_PATH)\n",
    "else:\n",
    "    model = load_model(DOWNLOAD_PATH)\n",
    "\n",
    "score = mnist_model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss (all 10 digits): %.4f'     % score[0])\n",
    "print('Test accuracy (all 10 digits): %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing networks with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "# Create directory for storing TensorBoard log files, if it doesn't exist already.\n",
    "# If it does, clear all logs currently in the directory.\n",
    "LOG_DIR=os.path.join(os.getcwd(), 'ann_keras_log_dir')\n",
    "print(\"Using\", LOG_DIR, \"as directory to store TensorBoard logs\")\n",
    "if not os.path.isdir(LOG_DIR):\n",
    "    os.mkdir(LOG_DIR)\n",
    "\n",
    "for f in os.listdir(LOG_DIR):\n",
    "    file_path = os.path.join(LOG_DIR, f)\n",
    "    try:\n",
    "        if os.path.isfile(file_path):\n",
    "            os.unlink(file_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# Create circles data\n",
    "X, classes = make_circles(n_samples=200, factor=0.3, noise=0.1)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "tboard = TensorBoard(log_dir=LOG_DIR, histogram_freq=0,\n",
    "                    write_graph=True, write_images=True)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.fit(X, classes, epochs=5, steps_per_epoch=512, verbose=0, callbacks=[tboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start a TensorBoard server, we would usually use the `tensorboard` command in our terminal. So for instance, if `LOG_DIR='/home/ann_keras_log_dir'` in the code above, I would go to my terminal/command line and write\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir /home/ann_keras_log_dir\n",
    "```\n",
    "\n",
    "to start TensorBoard. For convenience, I've added some Python code below that will do this for you. Run the following code cell and then go visit http://localhost:6006 in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start TensorBoard server\n",
    "from tensorboard import default\n",
    "from tensorboard import program\n",
    "\n",
    "tb = program.TensorBoard(default.get_plugins(), default.get_assets_zip_provider())\n",
    "tb.configure(argv=[None, '--logdir', LOG_DIR])\n",
    "tb.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional resources <a id=\"additional-resources\"></a>\n",
    "* [playground.tensorflow.org](https://playground.tensorflow.org/) allows you to experiment with some simple neural nets.\n",
    "* [Hacker's guide to Neural Networks](https://karpathy.github.io/neuralnets/)\n",
    "* References for specific ANN architectures:\n",
    "  * Convolutional networks\n",
    "  * LSTMs\n",
    "    * [Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Workshop-Kernel",
   "language": "python",
   "name": "workshop-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
