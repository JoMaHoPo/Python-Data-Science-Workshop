{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Introduction to Machine Learning in Python with Scikit-learn\n",
    "![LISA logo](https://raw.githubusercontent.com/wshand/Python-Data-Science-Workshop/master/assets/LISA_logo_medium.jpg)\n",
    "\n",
    "This notebook introduces some basic machine learning methods, and how to implement them using the scikit-learn library.\n",
    "\n",
    "# Table of contents\n",
    "* [Introduction](#introduction)\n",
    "* [Support vector machines](#svm)\n",
    "* [Normalizing data](#normalization)\n",
    "* [Principal components analysis](#pca)\n",
    "* [Scikit-learn pipelines](#pipelines)\n",
    "* [Cross-validation and grid search](#cross-validation)\n",
    "* [Additional references](#additional-references)\n",
    "\n",
    "# Introduction <a id=\"introduction\"></a>\n",
    "In recent years, Python has become a popular programming language for machine learning for a few reasons:\n",
    "\n",
    "* Python is a high-level language, and fairly easy to learn\n",
    "* It's relatively easy to integrate with low-level languages like C and Fortran, which can make well-written Python code run extremely quickly.\n",
    "* Python already had a lot of great libraries for data visualization and manipulation, like pandas and matplotlib\n",
    "\n",
    "In this workshop, we're going to use the [Labeled Faces in the Wild (LFW) dataset](http://vis-www.cs.umass.edu/lfw/). We'll use a subset of the data that includes about 1,300 images of seven individuals. Run the code cell below to download the data, and show a few of the people in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.3)\n",
    "\n",
    "# Get the number of images, as well as the height and width (in pixels) of each image\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "\n",
    "# Separate data into images and labels\n",
    "X = lfw_people.data\n",
    "y = lfw_people.target\n",
    "\n",
    "print(\"Number of faces:\", n_samples)\n",
    "print(\"Number of unique people:\", len(lfw_people.target_names))\n",
    "print(\"Height of every image:\", h, \"pixels\")\n",
    "print(\"Width of every image:\", w, \"pixels\")\n",
    "\n",
    "# Plot some randomly selected faces\n",
    "n_images_x, n_images_y = (5,3)\n",
    "fig, axes = plt.subplots(n_images_y, n_images_x, figsize=(12,8))\n",
    "rand_choices = np.random.randint(0, high=n_samples, size=n_images_x * n_images_y)\n",
    "\n",
    "for (kk, (ii, jj)) in zip(rand_choices, itertools.product(range(n_images_y), range(n_images_x))):\n",
    "    face = lfw_people.images[kk]\n",
    "    name = lfw_people.target_names[y[kk]]\n",
    "    axes[ii,jj].imshow(face, cmap='gray')\n",
    "    axes[ii,jj].set_xticks([])\n",
    "    axes[ii,jj].set_yticks([])\n",
    "    axes[ii,jj].set_xlabel(name)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector machines\n",
    "[SVM wiki article](https://en.wikipedia.org/wiki/Support_vector_machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing data <a id=\"normalization\"></a>\n",
    "[Normalization wiki article](https://en.wikipedia.org/wiki/Normalization_(statistics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal components analysis <a id=\"pca\"></a>\n",
    "Our images are 37 pixels tall by 28 pixels wide, and hence consist of 1,036 pixels each. This means that each data point we are using to train our support vector machine is 1,036-dimensional, which is fairly large. By reducing this dimension via *dimensionality reduction*, we are able to train our SVM much faster.\n",
    "\n",
    "One common way of performing dimensionality reduction is via [*principal components analysis*](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA). PCA finds the \"most important\" parts of the data (more specifically, it finds the directions in which the data vary the most), keeps those parts, and throws the rest away. As a demonstration, try out the code cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def square_axes(ax, data, expansion_factor=1.05):\n",
    "    # Change limits of plot axes to center on the input dataset, and to put the\n",
    "    # x-axis and y-axis on the same scale\n",
    "    m        = np.mean(data)\n",
    "    max_dist = max([np.linalg.norm(u-m) for u in data]) * expansion_factor\n",
    "    lims     = [m-max_dist, m+max_dist]\n",
    "    try:    ax.set_xlim(lims); ax.set_ylim(lims)\n",
    "    except: ax.xlim(lims); ax.ylim(lims)\n",
    "\n",
    "# Create some random data\n",
    "rdata = multivariate_normal.rvs(cov=[[4, 1], [1, 1]], size=200)\n",
    "\n",
    "# Fit PCA to the data\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(rdata)\n",
    "\n",
    "# TODO: Plot data and principal components\n",
    "plt.scatter(rdata[:,0], rdata[:,1], marker='x', color='k')\n",
    "square_axes(plt, rdata)\n",
    "plt.title(\"Some random data. Principal components shown as red arrows\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line              = np.max(np.abs(rdata)) * np.array([-pca.components_[0], pca.components_[0]])\n",
    "projected_lengths = rdata.dot(pca.components_[0])\n",
    "projections       = np.array([l * pca.components_[0] for l in projected_lengths])\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(8,16))\n",
    "\n",
    "# Plot data and line through first principal component in first subplot\n",
    "axes[0].scatter(rdata[:,0], rdata[:,1], marker='x', color='k')\n",
    "axes[0].plot(line[:,0], line[:,1], color='red', linewidth=4)\n",
    "axes[0].set_title(\"Original data, plus line in the direction of the first principal component\")\n",
    "\n",
    "# Plot data projected onto line in the last subplot\n",
    "axes[2].scatter(projected_data, [0 for d in projected_data], marker='x', color='k')\n",
    "axes[2].set_title(\"Data after transformation by PCA. Data are now one-dimensional instead of two-dimensional\")\n",
    "\n",
    "# Show the effect of the projection on the middle subplot using a few of the points. Points\n",
    "# are chosen randomly from those with the highest residual norm.\n",
    "c = sorted(range(rdata.shape[0]), key=lambda ii: np.linalg.norm(rdata[ii]-projections[ii]))\n",
    "c = np.array(c)[-np.random.choice(20, size=8)]\n",
    "axes[1].scatter(rdata[:,0], rdata[:,1], marker='x', color='k', alpha=0.2)\n",
    "axes[1].plot(line[:,0], line[:,1], color='red')\n",
    "axes[1].scatter(projections[c,0], projections[c,1], color='k')\n",
    "axes[1].scatter(rdata[c,0], rdata[c,1], marker='x', color='k')\n",
    "axes[1].set_title(\"Data are transformed by finding the closest point on the line\")\n",
    "for (orig,proj) in zip(rdata[c], projections[c]):\n",
    "    axes[1].plot([orig[0], proj[0]], [orig[1], proj[1]], 'r--')\n",
    "\n",
    "for (ax,dat) in zip(axes, (rdata, rdata[c], rdata)):\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    square_axes(ax,dat)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a more tangible idea of what PCA does, we can look at some of the [eigenfaces](https://en.wikipedia.org/wiki/Eigenface) from the LFW dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PCA. Show some of the eigenfaces\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "n_components=n_images_x * n_images_y\n",
    "pca = PCA(n_components=n_components).fit(X)\n",
    "\n",
    "# Show some eigenfaces\n",
    "fig, axes = plt.subplots(n_images_y, n_images_x, figsize=(12,8))\n",
    "for (kk, (ii,jj)) in enumerate(itertools.product(range(n_images_y), range(n_images_x))):\n",
    "    eigenface = pca.components_[kk].reshape((h,w))\n",
    "    axes[ii,jj].imshow(eigenface, cmap='gray')\n",
    "    axes[ii,jj].set_xticks([])\n",
    "    axes[ii,jj].set_yticks([])\n",
    "    axes[ii,jj].set_xlabel(\"#\" + str(kk + 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bringing everything together: Pipelines in scikit-learn <a id=\"pipelines\"></a>\n",
    "So far, in order to add new steps to our machine learning system we've had to define a variable for our data transformer/classifier, fit the machine learning algorithm, and then create new variables to pass on to the next step in the system. Keeping track of all of those variables can quickly get complicated. For that reason, scikit-learn includes the [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) to store the entire learning system from start to end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "\n",
    "learner = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('dim-reduce', PCA(n_components=100)),\n",
    "    ('classify', SVC(C=1.0, gamma='scale'))\n",
    "])\n",
    "\n",
    "learner.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation and grid search <a id=\"cross-validation\"></a>\n",
    "One of the most important parts of all machine learning algorithms is picking the best possible hyperparameters. The difference between a well-chosen and poorly-chosen hyperparameter can be staggering; *compare accuracy of methods after choosing C correctly*\n",
    "\n",
    "In order to improve our accuracy, we're going to try to find the best value of the hyperparameter `C` that the support vector machine uses. Recall that `C` *describe what C does*.\n",
    "\n",
    "The easiest way to choose `C` is to split our data into a *training set* and a *test set*. The training set is the part of the data that our machine learning algorithm actually learns from. Once the learner is trained, we score it on the test set, which gives a better estimate of how good our learner really is. After picking a few values of `C` and training our learner, we pick the value of `C` that got the best score on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "learner = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('dim-reduce', PCA(n_components=250)),\n",
    "    ('classify', SVC(C=1.0, gamma='scale', kernel='rbf'))\n",
    "])\n",
    "\n",
    "# Split into training and tests sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n",
    "\n",
    "# Fit the learner and score on the training and testing sets\n",
    "learner.fit(X_train, y_train)\n",
    "print(\"Score on training set: %.4f\" % learner.score(X_train, y_train))\n",
    "print(\"Score on testing set: %.4f\" % learner.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cross-validation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "learner = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('dim-reduce', PCA(n_components=100)),\n",
    "    ('classify', SVC(C=1.0, gamma='scale'))\n",
    "])\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "scores = []\n",
    "for (train, valid) in kf.split(X_train):\n",
    "    learner.fit(X_train[train], y_train[train])\n",
    "    scores.append(learner.score(X_train[valid], y_train[valid]))\n",
    "\n",
    "print(\"Average score on validation sets: %.4f\" % np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Grid search*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'classify__C': [0.5, 1.0, 1.5, 2.0, 2.5, 3.0]}\n",
    "grid_search = GridSearchCV(learner, parameters, cv=5)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional References <a id=\"additional-references\"></a>\n",
    "* O'Reilly books:\n",
    "* Since the boom of interest in deep learning over the past decade, multiple frameworks for writing neural networks have been developed:\n",
    "  * [TensorFlow](https://www.tensorflow.org)\n",
    "  * [PyTorch](https://www.pytorch.org)\n",
    "  * [Keras](), which isn't a standalone library but rather a high-level user interface that simplifies the process of writing neural nets in TensorFlow and PyTorch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Workshop-Kernel",
   "language": "python",
   "name": "workshop-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
