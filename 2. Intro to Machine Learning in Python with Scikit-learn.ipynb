{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Introduction to Machine Learning in Python with Scikit-learn\n",
    "![LISA logo](https://raw.githubusercontent.com/wshand/Python-Data-Science-Workshop/master/assets/LISA_logo_medium.jpg)\n",
    "\n",
    "This notebook introduces some basic machine learning methods and how to implement them using the scikit-learn library.\n",
    "\n",
    "# Table of contents\n",
    "* [Introduction](#introduction)\n",
    "* [Support vector machines](#svm)\n",
    "* [Normalizing data](#normalization)\n",
    "* [Scikit-learn pipelines](#pipelines)\n",
    "* [Principal components analysis](#pca)\n",
    "* [Cross-validation and grid search](#cross-validation)\n",
    "* [Additional references](#additional-references)\n",
    "\n",
    "# Introduction <a id=\"introduction\"></a>\n",
    "In recent years, Python has become a popular programming language for machine learning for a few reasons:\n",
    "\n",
    "* Python is a high-level language, and fairly easy to learn\n",
    "* It's relatively easy to integrate with low-level languages like C and Fortran, which can make well-written Python code run extremely quickly.\n",
    "* Python already had a lot of great libraries for data visualization and manipulation, like pandas and matplotlib\n",
    "\n",
    "Most cutting-edge, high-performance machine learning libraries have a Python interface. In this workshop, we're going to talk about one such library, [scikit-learn](https://scikit-learn.org). Scikit-learn is a toolbox with dozens upon dozens of the most popular algorithms for tasks like classification, regression, clustering, and dimensionality reduction. In addition, it contains utilities for cleaning up data and preparing it for machine learning.\n",
    "\n",
    "We're going to use the [Labeled Faces in the Wild (LFW) dataset](http://vis-www.cs.umass.edu/lfw/) to demonstrate how to use scikit-learn. The LFW dataset includes about 1,300 grayscale images of seven individuals. Run the code cell below to download the data and show a few of the images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.stats             import multivariate_normal\n",
    "from sklearn.decomposition   import PCA\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.svm             import SVC\n",
    "from sklearn.pipeline        import Pipeline\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.datasets        import fetch_lfw_people\n",
    "\n",
    "########## Helper functions\n",
    "def square_axes(ax, data, expansion_factor=1.05):\n",
    "    # Change limits of plot axes to center on the input dataset, and to put the\n",
    "    # x-axis and y-axis on the same scale\n",
    "    m        = np.mean(data)\n",
    "    max_dist = max([np.linalg.norm(u-m) for u in data]) * expansion_factor\n",
    "    lims     = [m-max_dist, m+max_dist]\n",
    "    try:    ax.set_xlim(lims); ax.set_ylim(lims)\n",
    "    except: ax.xlim(lims); ax.ylim(lims)\n",
    "\n",
    "def plot_decision_boundary(X, clf, ax, incr=1, h=.02):\n",
    "    # Plot the support vector machine decision boundary for 2D data\n",
    "    xmin, xmax = X[:,0].min()-incr, X[:,0].max()+incr\n",
    "    ymin, ymax = X[:,1].min()-incr, X[:,1].max()+incr\n",
    "    xx, yy     = np.meshgrid(np.arange(xmin,xmax,h),np.arange(ymin,ymax,h))\n",
    "    Z          = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=.2)\n",
    "##########################\n",
    "\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.3)\n",
    "\n",
    "# Get the number of images, as well as the height and width (in pixels) of each image\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "\n",
    "# Separate data into images and labels\n",
    "X = lfw_people.data\n",
    "y = lfw_people.target\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, shuffle=False)\n",
    "\n",
    "print(\"Number of faces:\", n_samples)\n",
    "print(\"Number of unique people:\", len(lfw_people.target_names))\n",
    "print(\"Height of every image:\", h, \"pixels\")\n",
    "print(\"Width of every image:\", w, \"pixels\")\n",
    "\n",
    "# Plot some randomly selected faces\n",
    "n_images_x, n_images_y = (5,3)\n",
    "fig, axes = plt.subplots(n_images_y, n_images_x, figsize=(12,8))\n",
    "rand_choices = np.random.choice(n_samples, size=n_images_x * n_images_y)\n",
    "\n",
    "for (kk, (ii, jj)) in zip(rand_choices, itertools.product(range(n_images_y), range(n_images_x))):\n",
    "    face = lfw_people.images[kk]\n",
    "    name = lfw_people.target_names[y[kk]]\n",
    "    axes[ii,jj].imshow(face, cmap='gray')\n",
    "    axes[ii,jj].set_xticks([])\n",
    "    axes[ii,jj].set_yticks([])\n",
    "    axes[ii,jj].set_xlabel(name)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector machines\n",
    "In this notebook, I'm going to show you how to use machine learning to recognize faces of people from the LFW data. By the end, you'll have a full-fledged machine learning system that can automatically classify these images by who they belong to.\n",
    "\n",
    "Our first step will be to understand [support vector machines](https://en.wikipedia.org/wiki/Support_vector_machine) (SVMs). Support vector machines are a powerful tool for classifying data, and have the benefit of being fairly simple to use. First we consider *linear SVMs*. Given a dataset and two classes (e.g. \"red points\" and \"blue points\", or \"image of Colin Powell\" and \"not an image of Colin Powell\"), a linear SVM attempts to draw a line (or, in higher dimensions, a hyperplane) between points in the two classes. This line is called the *decision boundary*, and all points on one side are said to be in the first class while the points on the other side are predicted to be in the second class.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wshand/Python-Data-Science-Workshop/master/assets/images/Linear%20SVM.jpg\" alt=\"SVM linear decision boundaries diagram\" style=\"width: 50%; height: 50%\">\n",
    "\n",
    "An SVM doesn't just pick any line between the two classes; it tries to find the line that is furthest from the data on each side. For datasets that are not separable (i.e. datasets where there doesn't exist a line that exactly separates the two classes), we make a tradeoff that is controlled by a hyperparameter $C$ that we choose. If $C$ is large, the SVM does its best to accurately classify points in both classes, but the line may have to get extremely close to both datasets. On the other hand, if $C$ is small, then we allow the SVM to make mistakes and misclassify some of the datapoints, but in return we get a decision boundary that cleanly splits the classes.\n",
    "\n",
    "A more sophisticated type of support vector machine is a *nonlinear SVM* (also called a kernel SVM or kernel machine). A kernel machine does the same thing as a linear SVM, but, using some fancy mathematics and a *kernel function*, allows for a more complicated decision boundary.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wshand/Python-Data-Science-Workshop/master/assets/images/Kernel%20SVM.jpg\" alt=\"SVM nonlinear decision boundary diagram\" style=\"width: 60%; height: 60%\">\n",
    "\n",
    "The following code shows how both linear support vector machines and nonlinear SVMs work in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "fig, axes = plt.subplots(2,2,figsize=(12,12))\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    ax.set_xticks([]); ax.set_yticks([]);\n",
    "\n",
    "### Linear SVM plots\n",
    "for (ax,std) in zip((axes[0,0], axes[1,0]), (0.3,1.0)):\n",
    "    # Create blob data\n",
    "    blobs, classes = make_blobs(centers=[[-1,1], [1,-1]], cluster_std=std)\n",
    "    ax.scatter(blobs[classes == 0,0], blobs[classes == 0,1])\n",
    "    ax.scatter(blobs[classes == 1,0], blobs[classes == 1,1])\n",
    "    \n",
    "    # Train support vector machine and plot decision boundary\n",
    "    clf = LinearSVC(); clf.fit(blobs, classes)\n",
    "    plot_decision_boundary(blobs, clf, ax)\n",
    "\n",
    "axes[0,0].set_title(\"For linearly separable data, SVM correctly classifies\\nboth blobs. \" +\n",
    "                   \"The decision boundary is chosen to be the\\nseparating line furthest away \" +\n",
    "                   \"from each blob.\")\n",
    "axes[1,0].set_title(\"Most data are not linearly separable. SVM makes a tradeoff between\\n\" +\n",
    "                 \"separating classes well and accurately classifying points\")\n",
    "\n",
    "### Nonlinear SVM plots\n",
    "moons, classes = make_moons(noise=.08)\n",
    "clf = SVC(kernel='rbf', gamma='scale', C=10); clf.fit(moons, classes)\n",
    "\n",
    "for ax in (axes[0,1],axes[1,1]):\n",
    "    ax.scatter(moons[classes == 0,0], moons[classes == 0,1])\n",
    "    ax.scatter(moons[classes == 1,0], moons[classes == 1,1])\n",
    "\n",
    "plot_decision_boundary(moons, clf, axes[1,1], incr=0.2, h=.005)\n",
    "axes[0,1].set_title(\"Nonlinear data. Though classes are distinct,\\n\" +\n",
    "                    \"clearly the boundary between them is nonlinear\")\n",
    "axes[1,1].set_title(\"Decision boundary with nonlinear SVM\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below, we have an artificial dataset with two pieces: a \"blob\" of points on the left in the first class, and a sort of crescent or $\">\"$ shape in the second class on the right. I've also flipped some of the points' labels (e.g. by randomly choosing to flip a point from class \"orange\" to class \"blue\") to add some noise to the data.\n",
    "\n",
    "Try changing `C` in the code below to see how it can affect your decision boundary. As you'll see, at `C = 1` the decision boundary looks fairly reasonable, cleanly separating the blob and crescent pieces. When `C` becomes too small the support vector machine chooses the \"simplest\" boundary possible; in fact, for `C` sufficiently small it'll just put every data point in the same class. At the other extreme, if you make `C` very large then your SVM will try to find an extremely convoluted decision boundary in order to accurately classify every point. That might get you great accuracy on the data you're training on, but in real life your machine learning system has to deal with data points it's never seen before. If your decision boundary is too complicated then you're more likely to incorrectly classify those data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles, make_classification\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,6))\n",
    "\n",
    "data, classes = make_classification(n_samples=200, n_features=2, n_redundant=0,\n",
    "                                    flip_y=0.06, random_state=1, class_sep=1.2)\n",
    "\n",
    "### Try changing the value of C!              ######\n",
    "### Some values of C to try: C = 1; C = .025; ######\n",
    "###  C = .01; C = 1000; C = 100000            ######\n",
    "\n",
    "# YOUR CODE HERE\n",
    "clf = SVC(C=1, kernel='rbf', gamma='scale')\n",
    "# END YOUR CODE\n",
    "\n",
    "####################################################\n",
    "\n",
    "clf.fit(data, classes)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.scatter(data[classes == 0,0], data[classes == 0,1])\n",
    "    ax.scatter(data[classes == 1,0], data[classes == 1,1])\n",
    "\n",
    "plot_decision_boundary(data, clf, axes[1], incr=0.1, h=0.005)\n",
    "axes[0].set_title(\"Example dataset\")\n",
    "axes[1].set_title(\"Decision boundary with C = \" + str(clf.C))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last couple of code cells, here's the approach we've taken to creating a support vector machine in scikit-learn:\n",
    "\n",
    "1. We import `SVC` (\"support vector classifier:) it from the `sklearn` module, if it wasn't imported previously:\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "```\n",
    "2. We then create an `SVC` object, and give some parameters. For instance, `C` is the hyperparameter $C$ that affects how \"clean\" our decision boundary is.\n",
    "```python\n",
    "clf = SVC(C=1, kernel='rbf', gamma='scale')\n",
    "```\n",
    "  `kernel='rbf'` tells scikit-learn that we want to use the [radial basis function kernel](https://en.wikipedia.org/wiki/Radial_basis_function_kernel), a popular kernel function for support vector machines. `gamma='scale'` causes `sklearn` to automatically choose a parameter $\\gamma$ that is required by the RBF kernel.\n",
    "3. We now call `fit` with the data we want to use to train the support vector machine:\n",
    "```python\n",
    "clf.fit(X,y) # X = dataset, y = classes\n",
    "```\n",
    "4. Now our support vector machine has been trained! We can make predictions using the `predict` function. For instance, if we want to classify some data stored in the variable `Z`, we run\n",
    "```python\n",
    "predicted_classes = clf.predict(Z)\n",
    "```\n",
    "\n",
    "Let's go through these steps on the LFW dataset. We're going to use scikit-learn's `train_test_split` function to put $85\\%$ of the data into a \"training set\" and the remaining $15\\%$ into a \"testing set\". We'll train the support vector machine on the training set and then see how well it does on the test set, which it won't be trained on. As we'll see, it will do much better at classifying faces from the training set than faces from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC # SVC = support vector classifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, \n",
    "                                                    shuffle=False, random_state=0)\n",
    "\n",
    "# 1. Create an SVC object. Give it the parameters we want to use for the classifier,\n",
    "#    e.g. the kind of kernel function to use, the value of the parameter C, etc.\n",
    "clf = SVC(C=1.0, gamma='scale', kernel='rbf')\n",
    "\n",
    "# 2. Call the fit function\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 3. Make predictions with our newly trained SVC\n",
    "predictions = clf.predict(X)\n",
    "\n",
    "# Show one correctly classified face and one incorrectly classified face\n",
    "idx_T = np.random.choice(np.where(predictions == y)[0])\n",
    "idx_F = np.random.choice(np.where(predictions != y)[0])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "for (ax,idx) in zip(axes, [idx_T,idx_F]):\n",
    "    ax.imshow(X[idx].reshape((h,w)), cmap='gray')\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.set_xlabel(\"Predicted: \" + lfw_people.target_names[predictions[idx]] +\n",
    "                 \"\\nActual: \"   + lfw_people.target_names[y[idx]])\n",
    "\n",
    "axes[0].set_title(\"Correctly classified\")\n",
    "axes[1].set_title(\"Incorrectly classified\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifiers like `SVC` offer a `score` function that tell you how accurate your machine learning system is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on training data: %.4f\" % clf.score(X_train, y_train))\n",
    "print(\"Accuracy on test data: %.4f\"     % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran this code I got the following output:\n",
    "\n",
    "> ```\n",
    "Accuracy on training set: 1.0000\n",
    "Accuracy on remaining data: 0.4278\n",
    "```\n",
    "\n",
    "The first line indicates that my support vector machine correctly identifies the person in the 500 images I trained it on (the *training data*) 100% of the time. However, on the remaining images (the *testing data*), I only got $42.78\\%$ accuracy. In other words, the SVM gets nearly $60\\%$ better accuracy on the images it was trained on than the images it wasn't trained on.\n",
    "\n",
    "That's not very good. In the real world, we want our machine learning systems to be able to deal with data they weren't trained on. For instance, suppose we wanted to teach a computer to play chess. Since there are over $10^{120}$ possible games of chess, we can only train our computer on a very small fraction of them. However, we still have to ensure that our machine learning system does well when playing positions it's never seen before.\n",
    "\n",
    "In the remainder of this notebook, we're going to look at how we can improve the accuracy of our SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing data <a id=\"normalization\"></a>\n",
    "[Normalization wiki article](https://en.wikipedia.org/wiki/Normalization_(statistics))\n",
    "\n",
    "Different features of datasets exist on a variety of scales. For example, consider the following data points (from the California housing dataset):\n",
    "\n",
    "['MedInc',\n",
    " 'HouseAge',\n",
    " 'AveRooms',\n",
    " 'AveBedrms',\n",
    " 'Population',\n",
    " 'AveOccup',\n",
    " 'Latitude',\n",
    " 'Longitude']\n",
    "\n",
    "*California housing dataset table*\n",
    "\n",
    "| (Statistic) | Median income | House age (years) | Average rooms | Population | Price |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| **Min**                | 0.5  | 1     | 0 | 0 | 0 | 0 |\n",
    "| **Max**                | 15.9 | 52    | 0 | 0 | 0 | 0 |\n",
    "| **Median**             | 3.5  | 29    | 0 | 0 | 0 | 0 |\n",
    "| **Average**            | 3.9  | 28.6  | 0 | 0 | 0 | 0 |\n",
    "| **Standard deviation** | 1.9  | 12.6  | 0 | 0 | 0 | 0 |\n",
    "\n",
    "The house ages range from 1 - 52 years; the populations from 3 - 35,682 people; and the prices from \\\\$??? - \\\\$???.\n",
    "\n",
    "$$\n",
    "X_{\\text{normalized}} = \\frac{X - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "Scikit-learn provides this functionality with the `StandardScaler` class. As we did with the support vector classifier, we first use the `fit` function provided by `StandardScaler` on the training data `X_train`. This computes the mean and standard deviation of the pixel intensity for every image in our training set so that we can normalize the data later. In order to perform the actual normalization, we use the `transform` function, e.g. `scaler.transform(X_train)` in order to normalize the images in the training set and `scaler.transformer(X_test)` to normalize the images in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Try training and scoring the support vector machine on the normalized dataset\n",
    "svc = SVC(C=1.0, gamma='scale', kernel='rbf')\n",
    "svc.fit(scaler.transform(X_train), y_train)\n",
    "\n",
    "print(\"Accuracy on training data: %.4f\" % svc.score(scaler.transform(X_train), y_train))\n",
    "print(\"Accuracy on test data: %.4f\"     % svc.score(scaler.transform(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran this code, I got the following results:\n",
    "\n",
    "> ```\n",
    "Accuracy on training set: 0.9653\n",
    "Accuracy on remaining data: 0.7990```\n",
    "\n",
    "That's a pretty remarkable improvement! Just by normalizing the data before training our support vector classifier, my classifier went from $43\\%$ accuracy in classifying the data it wasn't trained on to almost $80\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting pieces together: Pipelines in scikit-learn <a id=\"pipelines\"></a>\n",
    "Notice that in the previous section, training our support vector machine consisted of two steps:\n",
    "\n",
    "1. Normalizing the data with a `StandardScaler`\n",
    "2. Feeding the normalized data to the `SVC` object to train our learner\n",
    "\n",
    "As we add more and more steps, the overhead and boilerplate code required to keep track of each step and piece them together can become a nuisance. To help alleviate this problem, scikit-learn provides the `Pipeline` object ([docs](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)). To build a `Pipeline`, we must provide a list of the steps in our machine learning system as well as a name for those steps. For instance, in the code cell below I've named the `StandardScaler` step `'scaler'`, and the support vector classification step `'classify'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classify', SVC(C=1.0, gamma='scale', kernel='rbf'))\n",
    "])\n",
    "\n",
    "learner.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training data: %.4f\" % learner.score(X_train, y_train))\n",
    "print(\"Accuracy on test data: %.4f\"     % learner.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal components analysis <a id=\"pca\"></a>\n",
    "Our images are 37 pixels tall by 28 pixels wide, or 1,036 pixels total. This means that each data point we are using to train our support vector machine is 1,036-dimensional, which is fairly large. In this section, we'll reduce this dimension in order to train our SVM much faster.\n",
    "\n",
    "One common way of performing dimensionality reduction is via [*principal components analysis*](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA). PCA finds the \"most important\" parts of the data (more specifically, the directions of highest variance), known as the *principal components*. It keeps those parts and throws the rest of the data away.\n",
    "\n",
    "As a demonstration, try out the code cell below. In the code cell, we create some random data distributed according to a multivariate normal distribution. Then we generate some plots that show how the process of PCA works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create some random data\n",
    "rdata = multivariate_normal.rvs(cov=[[6, 1], [1, 0.5]], size=200)\n",
    "\n",
    "# Fit PCA to the data. Normally we would use n_components=1 to only keep the first principal component,\n",
    "# but here we'll use n_components=2 for purpose of demonstration\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(rdata)\n",
    "\n",
    "# Project data onto the first principal component\n",
    "line              = np.max(np.abs(rdata)) * np.array([-pca.components_[0], pca.components_[0]])\n",
    "projected_lengths = rdata.dot(pca.components_[0])\n",
    "projections       = np.array([l * pca.components_[0] for l in projected_lengths])\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(8,16))\n",
    "\n",
    "# __Plot 0__: Plot data and overlay principal components\n",
    "axes[0].scatter(rdata[:,0], rdata[:,1], marker='x', color='k', zorder=0, alpha=0.3)\n",
    "axes[0].quiver(0, 0, pca.components_[:,0] * np.sqrt(pca.singular_values_), \n",
    "           pca.components_[:,1] * np.sqrt(pca.singular_values_), color=['r', 'r'], zorder=1, scale=20)\n",
    "axes[0].set_title(\"Some random data. Principal components shown as red arrows\")\n",
    "\n",
    "# __Plot 1__: Plot data and line through first principal component in first subplot\n",
    "axes[1].scatter(rdata[:,0], rdata[:,1], marker='x', color='k')\n",
    "axes[1].plot(line[:,0], line[:,1], color='red', linewidth=4)\n",
    "axes[1].set_title(\"Original data, plus line in the direction of the first (most important) principal component\")\n",
    "\n",
    "# __Plot 2__: Show the effect of the projection on the middle subplot using a few of the points.\n",
    "# Points are chosen randomly from those with the highest residual norm.\n",
    "c = sorted(range(rdata.shape[0]), key=lambda ii: np.linalg.norm(rdata[ii]-projections[ii]))[-8:]\n",
    "axes[2].scatter(rdata[:,0], rdata[:,1], marker='x', color='k', alpha=0.2, zorder=0)\n",
    "axes[2].plot(line[:,0], line[:,1], color='red', zorder=1)\n",
    "axes[2].scatter(projections[c,0], projections[c,1], color='k', zorder=2)\n",
    "axes[2].scatter(rdata[c,0], rdata[c,1], marker='x', color='k', zorder=2)\n",
    "axes[2].set_title(\"Data are transformed by finding the closest point on the line\")\n",
    "for (orig,proj) in zip(rdata[c], projections[c]):\n",
    "    axes[2].plot([orig[0], proj[0]], [orig[1], proj[1]], 'r--', zorder=1)\n",
    "    \n",
    "# __Plot 3__: Plot data projected onto line in the last subplot\n",
    "axes[3].scatter(projected_lengths, [0 for l in projected_lengths], marker='o', \n",
    "                color='k', zorder=1, label='New, 1D data')\n",
    "axes[3].scatter(projected_lengths, rdata.dot([pca.components_[0,1] / pca.components_[0,0], -1]), \n",
    "                alpha=0.2, marker='x', color='k', zorder=1, label='Old, 2D data')\n",
    "axes[3].set_title(\"Data after transformation by PCA. Data are now one-dimensional instead of two-dimensional\")\n",
    "axes[3].legend()\n",
    "\n",
    "# Set axis ticks and axis limits for each plot\n",
    "for (ax,dat) in zip(axes, (rdata, rdata, rdata[c], rdata)):\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    square_axes(ax,dat)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a more tangible idea of what PCA does, we can look at some [eigenfaces](https://en.wikipedia.org/wiki/Eigenface) from our data. Each component found by running PCA on the LFW dataset is itself a 1,036-dimensional vector of pixel intensities, and hence can be represented as an image. This image can be interpreted as a \"template face\". Each face in the LFW dataset can then be reconstructed as a weighted sum of these templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try changing n_components to see how it affects the reconstruction of\n",
    "# the faces\n",
    "pca = PCA(n_components=150).fit(StandardScaler().fit_transform(X))\n",
    "\n",
    "fig, axes = plt.subplots(n_images_y, n_images_x, figsize=(12,8))\n",
    "for (kk, (ii,jj)) in enumerate(itertools.product(range(n_images_y), range(n_images_x))):\n",
    "    axes[ii,jj].imshow(pca.components_[kk].reshape((h,w)), cmap='gray')\n",
    "    axes[ii,jj].set_xticks([])\n",
    "    axes[ii,jj].set_yticks([])\n",
    "    axes[ii,jj].set_xlabel(\"#\" + str(kk + 1))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Project a few randomly chosen faces onto the principal components\n",
    "rand_choices = np.random.choice(X.shape[0], size=6) # 2 faces/row\n",
    "fig, axes = plt.subplots(3, 4, figsize=(12,8))\n",
    "for (ii,axrow) in enumerate(axes):\n",
    "    for (c,col) in zip(rand_choices[2*ii:2*(ii+1)],range(2)):\n",
    "        proj = pca.transform(X[c].reshape((1,h*w))).dot(pca.components_)\n",
    "        for (face,ax) in zip((X[c],proj), (axrow[2*col], axrow[2*col+1])):\n",
    "            ax.imshow(face.reshape((h,w)), cmap='gray')\n",
    "            ax.set_xticks([]); ax.set_yticks([])\n",
    "            ax.set_xlabel(lfw_people.target_names[y[c]])\n",
    "\n",
    "for ax in (axes[0,0], axes[0,2]): ax.set_title(\"Original face\")\n",
    "for ax in (axes[0,1], axes[0,3]): ax.set_title(\"Components retained by PCA\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run PCA in scikit-learn, we use the `PCA` class from `sklearn.decomposition`. We give it an `n_components` parameter that tells us how many principal components we want to keep. In the code cell below, for instance, by setting `n_components = 25` we're keeping the top 25 principal components (i.e. the 25 \"most important\" parts of the data), hence reducing the dimension of the data from 1,036 to just 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change n_components to change the number of principal components computed by PCA.\n",
    "#\n",
    "#     Higher n_components -> better retention of original image but higher dimension\n",
    "#     Lower n_components  -> worse retention of original image but lower dimension\n",
    "#\n",
    "# In the example of n_components = 25, we reduce the dimension of each data point/image\n",
    "# from 1,036 to 25.\n",
    "pca = PCA(n_components = 25)\n",
    "pca.fit(X_train)  # PCA doesn't need to know about the names (in the variable y) for each image\n",
    "X_pca = pca.transform(X_train)\n",
    "\n",
    "print(\"Dimension of data before PCA:\", X_train.shape[1])\n",
    "print(\"Dimension of data after PCA:\",  X_pca.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tradeoff in PCA is between how accurate our final learner will be and how quickly it can be trained. Lower `n_components` boosts speed at the cost of accuracy, and higher `n_components` sacrifices speed but can be significantly more accurate. The quality of this tradeoff depends on your dataset and what other learning algorithms you're using. If the features of your dataset are highly correlated you can get away with a small number of principal components. For the LFW dataset, I found that I needed fairly large values of `n_components` (500 or more) to get accuracy comparable to using the support vector classifier without PCA.\n",
    "\n",
    "Try running the code cell below with different values of `n_components` to see how it changes both the amount of training time as well as your accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Try changing n_components to see how it affects accuracy and training time\n",
    "learner = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('dim-reduce', PCA(n_components=100)), # <-- Change n_components here\n",
    "    ('classify', SVC(C=1.0, gamma='scale', kernel='rbf'))\n",
    "])\n",
    "\n",
    "tic = time.time()\n",
    "learner.fit(X_train, y_train)\n",
    "toc = time.time()\n",
    "\n",
    "print(\"Training time: %.4f seconds\"    % (toc - tic))\n",
    "print(\"Accuracy on training set: %.4f\" % learner.score(X_train, y_train))\n",
    "print(\"Accuracy on test set: %.4f\"     % learner.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation and grid search <a id=\"cross-validation\"></a>\n",
    "One of the most important parts in improving the performance of a machine learning algorithm is hyperparameter selection. Up until now, every time we've constructed an `SVC` object, we've passed `C = 1.0` to set the $C$ hyperparameter used by support vector machines. If we pick a different value of $C$, we may be able to improve our support vector machine's ability to accurately identify faces.\n",
    "\n",
    "To review, we want our learning system to do as well as possible on data it hasn't seen yet. A natural way of finding $C$ would thus be to repeatedly train a support vector classifier on `X_train` and `y_train` for different values of the hyperparameter, and then pick the one that scores the best on `X_test` and `y_test`. This approach is tried below, where we assess the performance of our SVM for all $C\\in\\{0.5,1,1.5,2,2.5\\}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for C in (0.5, 1.0, 1.5, 2.0, 2.5):\n",
    "    learner = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('dim-reduce', PCA(n_components=300)),\n",
    "        ('classify', SVC(C=C, gamma='scale', kernel='rbf'))\n",
    "    ])\n",
    "    learner.fit(X_train, y_train)\n",
    "    print(\"Score on testing set for C = %.2f: %.4f\" % (C,learner.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem with this approach is that as we keep tuning parameters, we start biasing ourselves towards the test set. In the example above, we search for the hyperparameter $C$ by choosing which value of $C$ does best on the test set. But if we keep fine-tuning $C$, we could run into a situation where we've found a hyperparameter that happens to do well on the training and test sets, but doesn't do well outside of these datasets. \n",
    "\n",
    "With just one hyperparameter, this isn't much of a problem. It's unlikely that a choice of $C$ that does well on the training and test sets will do poorly on other data. But for machine learning methods that require dozens or even hundreds of hyperparameters (such as neural networks), it's easy to overfit because you only used a single test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "for C in (0.5, 1, 1.5, 2, 2.5):\n",
    "    learner = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('dim-reduce', PCA(n_components=300)),\n",
    "        ('classify', SVC(C=C, gamma='scale', kernel='rbf'))\n",
    "    ])\n",
    "    scores = []\n",
    "    for (train, valid) in kf.split(X_train):\n",
    "        learner.fit(X_train[train], y_train[train])\n",
    "        scores.append(learner.score(X_train[valid], y_train[valid]))\n",
    "    print(\"Mean score on validation sets for C = %.2f: %.4f\" % (C, np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of having to loop over every value of the hyperparameters you want to check, scikit-learn provides the `GridSearchCV` class that automates this process for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "learner = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('dim-reduce', PCA(n_components=300)),\n",
    "    ('classify', SVC(kernel='rbf'))\n",
    "])\n",
    "\n",
    "parameters = {'classify__C': [0.5, 1.0, 1.5, 2.0, 2.5],\n",
    "             'classify__gamma': ['scale', 1e-4, 2e-4, 4e-4, 8e-4, 6e-3, 2e-2, 4e-2, 8e-2]}\n",
    "grid_search = GridSearchCV(learner, parameters, cv=5, verbose=2, refit=True, n_jobs=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# learner is now trained with the best value of the hyperparameters\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(\"Accuracy on test set with best C: %.2f\" % grid_search.best_estimator_.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a learner using the best parameters found in the previous cell and\n",
    "# without PCA in order to get the most accurate learner possible\n",
    "params = grid_search.best_params_\n",
    "C      = params['classify__C'] if 'classify__C' in params else 1.0\n",
    "gamma  = params['classify__gamma'] if 'classify__gamma' in params else 'scale'\n",
    "\n",
    "learner = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classify', SVC(C=C, gamma=gamma, kernel='rbf'))\n",
    "])\n",
    "\n",
    "learner.fit(X_train, y_train)\n",
    "acc = learner.score(X_test, y_test)\n",
    "print(\"Accuracy on test set with best parameters and no PCA: %.4f\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional References <a id=\"additional-references\"></a>\n",
    "* [Scikit-learn documentation](https://scikit-learn.org/stable/)\n",
    "  * The [API reference](https://scikit-learn.org/stable/modules/classes.html) provides a thorough list of all the utilities and algorithms provided by `sklearn`.\n",
    "* O'Reilly books:\n",
    "  * [Hands-On Machine Learning with Scikit-Learn & TensorFlow](http://shop.oreilly.com/product/0636920052289.do)\n",
    "* Although scikit-learn provides neural networks via `sklearn.neural_networks`, the models it provides are very limited and not especially competitive. Instead, if you're interested in creating a neural net you should try one of the following libraries/APIs:\n",
    "  * [TensorFlow](https://www.tensorflow.org)\n",
    "  * [PyTorch](https://www.pytorch.org)\n",
    "  * [Keras](https://keras.io/)\n",
    "* [An Idiot's Guide to Support Vector Machines](http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf)\n",
    "  * [Accompanying discussion on Hacker News](https://news.ycombinator.com/item?id=18794545)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Workshop-Kernel",
   "language": "python",
   "name": "workshop-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
